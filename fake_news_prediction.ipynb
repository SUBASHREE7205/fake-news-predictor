{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdv7dgGwXhfZkuFn8NgEVk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SUBASHREE7205/fake-news-predictor/blob/main/fake_news_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5nj0qTsjHLG",
        "outputId": "e762105c-db1c-4f98-f6c9-fad269703710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk numpy pandas  scikit-learn tensorflow flask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # For evaluation\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "_gzNhQSCjmoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Sample data to simulate a larger dataset\n",
        "real_news = [\n",
        "    \"NASA confirms the discovery of water on Mars.\",\n",
        "    \"Government announces a new healthcare policy for citizens.\",\n",
        "    \"Stock market hits record high this week amid growth.\",\n",
        "    \"Scientists discover new method to fight cancer.\",\n",
        "    \"New technology advancements in artificial intelligence.\"\n",
        "]\n",
        "\n",
        "fake_news = [\n",
        "    \"Win a free iPhone by clicking this link!\",\n",
        "    \"Breaking: Celebrity caught in a fake scandal goes viral.\",\n",
        "    \"Click here to claim your free Bitcoin now!\",\n",
        "    \"Exclusive: You’ve just won $1,000,000 - Act fast!\",\n",
        "    \"Shocking news: Aliens land in the middle of Times Square!\"\n",
        "]\n",
        "\n",
        "# Create a dataset by repeating the real and fake news examples\n",
        "text_data = real_news * 200 + fake_news * 200  # Repeating each news type 200 times\n",
        "\n",
        "# Labels: 0 for real news and 1 for fake news\n",
        "labels = [0] * 200 + [1] * 200  # Repeating the labels for real and fake news\n",
        "\n",
        "\n",
        "dataset = list(zip(text_data, labels))\n",
        "random.shuffle(dataset)\n",
        "\n",
        "\n",
        "\n",
        "df_large = pd.DataFrame(dataset, columns=['text', 'label'])\n",
        "\n",
        "\n",
        "print(\"Dataset Preview:\")\n",
        "print(df_large.head())\n",
        "\n",
        "\n",
        "print(\"\\nDataset Information:\")\n",
        "print(df_large.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA6vqKLSj3_b",
        "outputId": "a773a408-f183-4cda-9afa-4c798a21c637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Preview:\n",
            "                                                text  label\n",
            "0  Government announces a new healthcare policy f...      1\n",
            "1  New technology advancements in artificial inte...      0\n",
            "2      NASA confirms the discovery of water on Mars.      0\n",
            "3  New technology advancements in artificial inte...      1\n",
            "4  Government announces a new healthcare policy f...      1\n",
            "\n",
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 400 entries, 0 to 399\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    400 non-null    object\n",
            " 1   label   400 non-null    int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 6.4+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')       # For tokenizing words\n",
        "nltk.download('stopwords')   # For removing common stopwords\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_QYTHq7kC6l",
        "outputId": "9e564cf2-841a-4c10-de4f-4911027deb1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download 'punkt' correctly\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download stopwords to ensure text cleaning is complete\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjeDEZuGkIC_",
        "outputId": "20a38a29-8664-4d5e-d180-c02820ca4144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.data.path.append('/usr/local/share/nltk_data')\n",
        "\n"
      ],
      "metadata": {
        "id": "l-7M1UVPkKgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLTK (if you haven't already)\n",
        "!pip install nltk\n",
        "\n",
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4QgG1SakP3e",
        "outputId": "50df34a9-7bda-48e4-ead7-bab7b181032c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    words = [word for word in words if word not in stop_words]\n",
        ")\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n"
      ],
      "metadata": {
        "id": "uzPa7ZEYkU1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Force download the 'punkt' resource correctly\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Also download 'stopwords' if needed\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHgAHV1qkaks",
        "outputId": "78ebdabd-7d6a-41f8-eea0-a36b5d52d9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Add the correct NLTK data path\n",
        "nltk.data.path.append('/usr/local/share/nltk_data')\n",
        "\n",
        "# Now download punkt explicitly\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbIa0pq_ksaJ",
        "outputId": "08ed91fc-3efb-413d-a868-ce4640021499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Now proceed with your existing code:\n",
        "# ... (your code for preprocessing and applying to df_large) ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7I-pqfPlrr9",
        "outputId": "01e3371f-698e-443a-cd5a-93bdb7cb8ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the preprocessing function to the 'text' column\n",
        "df_large['cleaned_text'] = df_large['text'].apply(preprocess_text)\n",
        "\n",
        "# Show the first few rows of the original and cleaned text\n",
        "print(df_large[['text', 'cleaned_text']].head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9PUVUGPk2Z1",
        "outputId": "b0d7aacd-5873-461c-df3d-2a844972f2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  Government announces a new healthcare policy f...   \n",
            "1  New technology advancements in artificial inte...   \n",
            "2      NASA confirms the discovery of water on Mars.   \n",
            "3  New technology advancements in artificial inte...   \n",
            "4  Government announces a new healthcare policy f...   \n",
            "\n",
            "                                  cleaned_text  \n",
            "0  govern announc new healthcar polici citizen  \n",
            "1       new technolog advanc artifici intellig  \n",
            "2             nasa confirm discoveri water mar  \n",
            "3       new technolog advanc artifici intellig  \n",
            "4  govern announc new healthcar polici citizen  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "metadata": {
        "id": "GGL3peHpl8vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust the number of features if needed\n"
      ],
      "metadata": {
        "id": "nY-H6F5pl-8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model and transform the cleaned text into TF-IDF features\n",
        "X = tfidf_vectorizer.fit_transform(df_large['cleaned_text']).toarray()\n",
        "\n",
        "# Get the target labels (0 for real, 1 for fake)\n",
        "y = df_large['label']"
      ],
      "metadata": {
        "id": "6ZZSNyxOmChx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the shape of the TF-IDF matrix\n",
        "print(\"Shape of the TF-IDF feature matrix:\", X.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4JIQtQ8mIqi",
        "outputId": "81a0ed86-7373-4037-ad4f-fe03eaf63aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the TF-IDF feature matrix: (400, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shapes of the training and testing sets\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMz_sjoBmMa7",
        "outputId": "8ff4e27e-7dde-4597-c133-e7c9cf84000d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (320, 28)\n",
            "Testing data shape: (80, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpE0KofHmQtC",
        "outputId": "14d88e3f-7d90-4faf-ae1a-31737f4ec759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Classification Report (precision, recall, F1-score)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcBP322vmaaO",
        "outputId": "62ecc928-6737-4e04-d3bf-f7c8655cf9ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 40.00%\n",
            "Confusion Matrix:\n",
            "[[ 6 38]\n",
            " [10 26]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.14      0.20        44\n",
            "           1       0.41      0.72      0.52        36\n",
            "\n",
            "    accuracy                           0.40        80\n",
            "   macro avg       0.39      0.43      0.36        80\n",
            "weighted avg       0.39      0.40      0.34        80\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example new article text\n",
        "new_article = [\"NASA successfully lands on Mars, a historic achievement.\"]\n",
        "\n",
        "# Preprocess the new article\n",
        "new_article_cleaned = preprocess_text(new_article[0])\n",
        "\n",
        "# Vectorize the new article using the trained TF-IDF vectorizer\n",
        "new_article_tfidf = tfidf_vectorizer.transform([new_article_cleaned]).toarray()\n",
        "\n",
        "# Predict if the new article is real (0) or fake (1)\n",
        "prediction = model.predict(new_article_tfidf)\n",
        "\n",
        "print(f\"Prediction: {'Real' if prediction[0] == 0 else 'Fake'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePeFBWRumctg",
        "outputId": "4e353d27-7bd6-4741-e2ac-ff3f2ecd13e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Fake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example new article text\n",
        "new_article = [\"NASA successfully lands on Mars, a historic achievement.\"]\n",
        "\n",
        "# Preprocess the new article\n",
        "new_article_cleaned = preprocess_text(new_article[0])\n",
        "\n",
        "# Vectorize the new article using the trained TF-IDF vectorizer\n",
        "new_article_tfidf = tfidf_vectorizer.transform([new_article_cleaned]).toarray()\n",
        "\n",
        "# Predict if the new article is real (0) or fake (1)\n",
        "prediction = model.predict(new_article_tfidf)\n",
        "\n",
        "print(f\"Prediction: {'Real' if prediction[0] == 0 else 'Fake'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUxXb0SiooAg",
        "outputId": "2bf56af1-4a94-4e55-d063-7ddebfe2711f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Fake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Predict new article\n",
        "\n",
        "# New article you want to test (copy and paste the article text here)\n",
        "new_article = [\"NASA’s Perseverance rover successfully landed on Mars on February 18, 2021, after a seven-month journey through space. The rover will search for signs of ancient life and collect samples to be returned to Earth.\"]\n",
        "\n",
        "# Preprocess the new article using the same preprocessing function you defined earlier\n",
        "new_article_cleaned = preprocess_text(new_article[0])\n",
        "\n",
        "# Vectorize the new article using the trained TF-IDF vectorizer\n",
        "new_article_tfidf = tfidf_vectorizer.transform([new_article_cleaned]).toarray()\n",
        "\n",
        "# Predict whether the new article is real (0) or fake (1)\n",
        "prediction = model.predict(new_article_tfidf)\n",
        "\n",
        "# Display the prediction\n",
        "if prediction[0] == 0:\n",
        "    print(\"Prediction: Real\")\n",
        "else:\n",
        "    print(\"Prediction: Fake\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXlfX77rotKK",
        "outputId": "dfc456ee-dd2a-43ad-cfbc-710da86d2bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Fake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example list of articles\n",
        "articles = [\n",
        "    \"NASA’s Perseverance rover successfully landed on Mars on February 18, 2021, after a seven-month journey through space.\",\n",
        "    \"Click here to win a free iPhone by clicking this link now!\",\n",
        "    \"Scientists have discovered a new planet that could support life.\"\n",
        "]\n",
        "\n",
        "for article in articles:\n",
        "    article_cleaned = preprocess_text(article)\n",
        "    article_tfidf = tfidf_vectorizer.transform([article_cleaned]).toarray()\n",
        "    prediction = model.predict(article_tfidf)\n",
        "    print(f\"Article: {article}\\nPrediction: {'Real' if prediction[0] == 0 else 'Fake'}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-TGaeeAou0-",
        "outputId": "7fd27bc0-a818-4cf4-fdf2-e82338d49446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article: NASA’s Perseverance rover successfully landed on Mars on February 18, 2021, after a seven-month journey through space.\n",
            "Prediction: Fake\n",
            "\n",
            "Article: Click here to win a free iPhone by clicking this link now!\n",
            "Prediction: Fake\n",
            "\n",
            "Article: Scientists have discovered a new planet that could support life.\n",
            "Prediction: Fake\n",
            "\n"
          ]
        }
      ]
    }
  ]
}